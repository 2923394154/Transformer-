# 特征与因子的关系说明

## 概念定义

### 特征 (Features)
- **定义**: 从原始数据中直接提取或简单计算得到的可观测变量
- **特点**: 具有明确的数据来源和计算方法，通常是模型的直接输入
- **性质**: 相对原始，可解释性强

### 因子 (Factors)
- **定义**: 经过处理、组合或变换后的特征，具有明确的金融意义
- **特点**: 用于解释资产收益率的驱动力，通常是特征的复杂组合
- **性质**: 具有预测能力，是投资决策的依据

## 在我们系统中的关系

### 第一层：原始特征
```python
# 6大基本特征（等量K线提取）
feature_cols = ['open', 'high', 'low', 'close', 'vwap', 'amount']
```

**特征来源：**
1. **open** - 等量K线开盘价
2. **high** - 等量K线最高价  
3. **low** - 等量K线最低价
4. **close** - 等量K线收盘价
5. **vwap** - 成交量加权平均价
6. **amount** - 成交金额

### 第二层：特征工程
```python
# 数据标准化
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# 时间序列构建
dataset = FactorDataset(features_scaled, labels, seq_len=40)
```

**处理过程：**
- **标准化**: 消除量纲影响
- **时间窗口**: 构建40个交易日的序列
- **序列化**: 将特征转换为时间序列输入

### 第三层：因子合成（Transformer处理）
```python
class TransformerEncoder(nn.Module):
    def forward(self, src):
        # Input Embedding: 特征 -> 高维表示
        src = self.input_embedding(src) * math.sqrt(self.d_model)
        
        # Positional Encoding: 添加时间信息
        src = self.pos_encoding(src)
        
        # Multi-Head Attention: 学习特征间关系
        output = self.transformer_encoder(src)
        
        # Output Projection: 生成合成因子
        prediction = self.output_projection(last_output)
        
        return prediction
```

**因子合成过程：**
1. **特征嵌入**: 6维特征映射到128维高维空间
2. **位置编码**: 添加时间序列信息
3. **注意力机制**: 学习特征间的复杂依赖关系
4. **前馈网络**: 进一步特征变换
5. **输出投影**: 生成最终的合成因子

### 第四层：目标因子
```python
# 预测未来10日收益率
stock_prices['future_return'] = stock_prices['close'].pct_change(10).shift(-10)
```

## 数据流转关系

```
原始股票数据 → 等量K线构建 → 6大基本特征 → 特征标准化 → 时间序列 → Transformer → 合成因子 → 收益率预测
    ↓               ↓              ↓           ↓           ↓          ↓         ↓          ↓
  OHLC+Vol+Amt   等量划分      原始特征     工程特征    序列特征   隐含因子   合成因子   预测因子
```

## 特征与因子的层次关系

### Level 1: 原始特征 (Raw Features)
- **数据**: OHLC价格、成交量、成交金额
- **特点**: 直接可观测，易于理解
- **用途**: 模型的基础输入

### Level 2: 工程特征 (Engineered Features)  
- **数据**: 标准化后的特征、时间序列特征
- **特点**: 经过预处理，适合模型训练
- **用途**: 提高模型性能

### Level 3: 隐含因子 (Latent Factors)
- **数据**: Transformer中间层的高维表示
- **特点**: 不可直接观测，包含复杂模式
- **用途**: 捕捉特征间的非线性关系

### Level 4: 合成因子 (Composite Factor)
- **数据**: 模型最终输出的预测值
- **特点**: 综合所有特征信息的因子
- **用途**: 直接用于收益率预测

## 因子合成的核心机制

### 1. 多头注意力 (Multi-Head Attention)
```python
# 学习不同特征间的关系权重
attention_weights = softmax(Q @ K.T / sqrt(d_k))
output = attention_weights @ V
```
- **作用**: 动态学习特征重要性
- **效果**: 不同时点、不同特征的权重自适应调整

### 2. 时间依赖建模
```python
# 位置编码 + 序列建模
pe[:, 0::2] = sin(position * div_term)
pe[:, 1::2] = cos(position * div_term)
```
- **作用**: 捕捉时间序列中的趋势和周期
- **效果**: 模型理解特征的时间演化模式

### 3. 非线性变换
```python
# 前馈网络进行非线性映射
ffn_output = activation(linear2(linear1(x)))
```
- **作用**: 发现特征间的非线性关系
- **效果**: 构建复杂的因子组合

## IC损失函数的作用

```python
def ic_loss(predictions, targets):
    # 计算预测值与真实值的相关系数
    ic = correlation(predictions, targets)
    return -ic  # 最大化IC
```

**损失函数的意义：**
- **优化目标**: 最大化信息系数(IC)
- **经济含义**: 提高因子的预测能力
- **效果**: 确保合成因子与未来收益率高度相关

## 实际应用中的特征-因子转换

### 输入端 (特征)
```python
# 某个时点的6维特征向量
features = [open_price, high_price, low_price, close_price, vwap, amount]
```

### 输出端 (因子)
```python
# 合成因子（标量）
composite_factor = transformer_model(feature_sequence)
```

### 预测应用
```python
# 基于因子进行投资决策
if composite_factor > threshold:
    action = "买入"
elif composite_factor < -threshold:
    action = "卖出"
else:
    action = "持有"
```

## 总结

1. **特征是基础**: 6大基本特征提供了原始信息
2. **因子是目标**: 通过模型学习得到的合成因子具有预测能力
3. **转换是过程**: Transformer实现了从特征到因子的智能转换
4. **应用是目的**: 合成因子最终用于投资决策和收益预测

这种设计实现了从"可观测特征"到"预测因子"的端到端学习，充分发挥了深度学习在因子挖掘中的优势。 